# Lab 2 for KTH course ID2223 Scalable Machine Learning and Deep Learning

For this lab assignment we performed Parameter Efficient Fine-Tuning (PEFT) with LoRA of a Large Language Model on a GPU.

## Task 2
(a) model-centric ways to improve
(b) data-centric ways to improve include using a bigger or better dataset to train on. For one, we found a (deduplicated version)[https://huggingface.co/datasets/mlabonne/FineTome-100k-dedup] of the Finetome 100K dataset we used which is a pretty straight-forward way to get "better" data. Another idea we discussed was getting a less generalised dataset of comparable size to create a more specialised chatbot.

## Comparing different foundation LLMS finetuned
